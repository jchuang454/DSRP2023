paired is only true when comparing the same group in two different points in time

t = samplemean-populationmean/samplestandarddeviation/sqrsamplesize

what if we have more than 2 samples?

anova = ANalsis Of VAriance

f statistic = variance between groups/ variance within groups

null hypothesis: all groups have the same mean

alternatve hypothesis: 2+ groups have different means

aov(data = dataset, numeri_var \~ cat_var)

dataset is the name of the dataset with the columns you want to analyze

numeric_var is the numeric variable you want to use to test for a difference

cat_var is the categorical variable that you want to compare the groups of

we can think of the \~ symbol as a "by" symbol

finding meaning of anova in r

anova_test \<- aov(data = dataset,numeric_var \~ cat_var

summary(anova_test)

TukeyHSD(anova_test). this tells us which categories are different

## machine learning:

at its core its a bunch of math and equations and it uses the equations to make predictions

supervised vs unsupervised learning

supervised: we give the data labels, we are going to spend most of our time on this, used for classification and regression

unsupervised: computer working things out on its own

supervised: data is labled, we split data into testing and training sets, used for prediction (what i did with pandas i think with test_train_split)

unsupervised: data is unlabeled, uses all the data at once, used for discovery

dimensionality reduction and k means clustering

dimensionality: we plot in 2 dimensions, one variable by another variable

we reduce data from 3d to 2d and we rotate the edges

principal component analysis (pca), captures 99.46% of total variation

## unsupervised learning

principal components analysis

```{r}
head(iris)
library(ggplot2)
library(dplyr)
library(corrplot)

#3remove any non-numeric variables
iris_num <- select(iris,-Species)
iris_num

## do pca
pcas <- prcomp(iris_num,scale. = T)
pcas
summary(pcas)
pcas$rotation

pcas$rotation^2

# get the x values of pcas and make it a data frame
pca_vals <- as.data.frame(pcas$x)
pca_vals$Species <- iris$Species
ggplot(pca_vals,aes(PC1,PC2, color = Species))+
  geom_point()+
  theme_minimal()
# setosa tends to be on its own while 
#versicolor and virginica tend to be together
# pc values are principal components and are 
#different axis that we slice the data with to find 
#where the most variation is
```

### k means clustering

if we have a set number of groups and we want to see how well the data falls into these groups

every time you run it will be different unless you tell it to start at the same point

unlabelled data into labelled clusters with centroids

x and y axis do not change

if you give it 3 it makes 3 groups

user: input number of clusters(k)

algorithim:

-   k random points chosen as cluster means

-   every point classified into closest cluster

-   recalculate mean for each cluster and repeat

remove categorical variables and na values and scale data

k \<- kmeans(dataset\$column, centers = num_of_clusters)

can cherry pick clusters to use with kmeans

p value below 0.05 is significant, reject null (for t test)
